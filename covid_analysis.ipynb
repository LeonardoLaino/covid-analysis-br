{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Projeto, Data Understanding & Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 - Definição do Projeto & Motivação"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> No dia 05/05/2023 a Organização Mundial da Saúde (OMS) declarou o fim da Emergência de Saúde Pública de Importância Internacional referente à COVID-19. [Fonte :OMS](https://www.paho.org/pt/noticias/5-5-2023-oms-declara-fim-da-emergencia-saude-publica-importancia-internacional-referente). \n",
    ">\n",
    "> Este anúncio traz alivio e fim a 3 longos anos, marcados por muita incerteza, perdas e mudanças talvez incomensuráveis na sociedade e no relacionamento humano.\n",
    ">\n",
    "> `Este projeto visa analisar os dados da COVID 19 no Brasil e extrair insights de como a mesma se comportou no país nos últimos 3 anos.`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Sobre os Dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Neste projeto, irei utilizar os dados coletados e compilados pelo Centro de Ciência de Sistemas e Engenharia da universidade americana **John Hopkins** ([link](https://www.jhu.edu)). Os dados são atualizados diariamente deste janeiro de 2020 com uma granularidade temporal de dias e geográfica de regiões de países (estados, condados, etc.). O site do projeto pode ser acessado neste [link](https://systems.jhu.edu/research/public-health/ncov/) enquanto os dados, neste [link](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports). Abaixo estão descritos os dados derivados do seu processamento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **date**: data de referência;\n",
    " - **state**: estado;\n",
    " - **country**: país; \n",
    " - **population**: população estimada;\n",
    " - **confirmed**: número acumulado de infectados;\n",
    " - **confirmed_1d**: número diário de infectados;\n",
    " - **confirmed_moving_avg_7d**: média móvel de 7 dias do número diário de infectados;\n",
    " - **confirmed_moving_avg_7d_rate_14d**: média móvel de 7 dias dividido pela média móvel de 7 dias de 14 dias atrás;\n",
    " - **deaths**: número acumulado de mortos;\n",
    " - **deaths_1d**: número diário de mortos;\n",
    " - **deaths_moving_avg_7d**: média móvel de 7 dias do número diário de mortos;\n",
    " - **deaths_moving_avg_7d**: média móvel de 7 dias dividido pela média móvel de 7 dias de 14 dias atrás;\n",
    " - **month**: mês de referência;\n",
    " - **year**: ano de referência."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Sobre o Autor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Olá!\n",
    ">\n",
    "> Me chamo Leonardo, sou estudante do último ano de Engenharia Mecânica que tem como objetivo profissional migrar para a área de dados. Durante os anos de faculdade, trabalhei durante 2 anos com pesquisa na área aeroespacial (lançadores de satélite) no Instituto de Aeronáutica e Espaço, em São José dos Campos (IAE-DCTA). Lá tive meu primeiro contato com a programação e com dados (maioria proveniente de sensores acoplados aos foguetes) e, unindo essa experiencia com meu interesse pelas aulas de estatística durante a graduação, acabei conhecendo a área de ciencia de dados e me apaixonando."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta, datetime\n",
    "from typing import Iterator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extração dos Dados"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Realizando a Extração\n",
    "\n",
    "<font color = red><b>Atenção:</b></font> Esta seção não precisa ser executada. Para rodar o notebook, realizar o carregamento do arquivo utilizando o Pandas através do arquivo `filtered-raw-data.pkl`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Os dados estão salvos no formato `.csv`, onde cada arquivo corresponde a um dia do ano."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criando um `Generator` para criar nosso `Iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range(data_inicial: datetime, data_final: datetime) -> Iterator:\n",
    "\n",
    "    \"\"\"\n",
    "    Generator.\n",
    "    Essa função recebe duas datas como input (uma data inicial e outra final), criando um Iterator\n",
    "    com todas as datas entre elas. \n",
    "    A granularidade é em dias.\n",
    "\n",
    "    Exemplo:\\n\\n\n",
    "    >> Input\\n\n",
    "    data_inicial = 01/01/2021\\n\n",
    "    data_final = 05/01/2021\\n\n",
    "    date_range(data_inicial, data_final)\\n\n",
    "    \\n\n",
    "    >> Output\\n\n",
    "    Iterator contendo os valores: 01/01/2021, 02/01/2021, 03/01/2021, 04/01/2021 e 05/01/2021.\n",
    "    \"\"\"\n",
    "    # Calculando a quantidade de dias\n",
    "    days_range = (data_final - data_inicial).days\n",
    "\n",
    "    # Criando o Iterator\n",
    "    for lag in range(days_range):\n",
    "        yield data_inicial + timedelta(lag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setando a data inicial e final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inicial = datetime(2021, 1, 1)\n",
    "data_final = datetime(2023, 1, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extraindo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para salvar as datas que não foram carregadas\n",
    "failed = list()\n",
    "\n",
    "# Objeto para criar o data frame Pandas\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Iterando sob todas as datas entre o intervalo que queremos considerar\n",
    "for date in date_range(data_inicial = data_inicial, data_final = data_final):\n",
    "    # Convertendo a data para string\n",
    "    date_str = date.strftime('%m-%d-%Y')\n",
    "\n",
    "    # Acessando o .csv através da url\n",
    "    source_url = f'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/{date_str}.csv'\n",
    "\n",
    "    \n",
    "    try: # Tenta acessar o arquivo referente à data da iteração atual\n",
    "\n",
    "        case = pd.read_csv(source_url, sep=',')\n",
    "\n",
    "    except: # Caso dê algum erro (possivelmente o arquivo não exista)\n",
    "\n",
    "        failed.append(date_str)\n",
    "        continue\n",
    "\n",
    "    else: # Caso consiga ler com sucesso o arquivo, adicionamos ao data frame\n",
    "\n",
    "        # Removendo as colunas que não iremos utilizar\n",
    "        case = case.drop(['FIPS', 'Admin2', 'Last_Update', 'Lat', 'Long_', 'Recovered', 'Active', 'Combined_Key', 'Case_Fatality_Ratio'], axis=1)\n",
    "        # Filtrando apenas os dados do Brasil\n",
    "        case = case.query('Country_Region == \"Brazil\"').reset_index(drop= True)\n",
    "        # Convertendo a coluna de Data\n",
    "        case['Date'] = pd.to_datetime(date.strftime('%Y-%m-%d'))\n",
    "        # Inserindo os dados dessa iteração ao data frame\n",
    "        df = pd.concat([df, case], axis= 0, ignore_index = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uma vez que o dataframe extraído é pequeno (e o processo de extração é relativamente demorado), irei exporta-lo em `.csv` caso haja necessidade de resetar o kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./filtered-raw-data.pkl', protocol= 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Checando os Resultados da Extração"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./filtered-raw-data.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Checando o resultado da extração dos dados (shape, dtypes e nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province_State</th>\n",
       "      <th>Country_Region</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "      <th>Incident_Rate</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>41689</td>\n",
       "      <td>796</td>\n",
       "      <td>4726.992352</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alagoas</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>105091</td>\n",
       "      <td>2496</td>\n",
       "      <td>3148.928928</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amapa</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>68361</td>\n",
       "      <td>926</td>\n",
       "      <td>8083.066602</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazonas</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>201574</td>\n",
       "      <td>5295</td>\n",
       "      <td>4863.536793</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bahia</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>494684</td>\n",
       "      <td>9159</td>\n",
       "      <td>3326.039611</td>\n",
       "      <td>2021-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province_State Country_Region  Confirmed  Deaths  Incident_Rate       Date\n",
       "0           Acre         Brazil      41689     796    4726.992352 2021-01-01\n",
       "1        Alagoas         Brazil     105091    2496    3148.928928 2021-01-01\n",
       "2          Amapa         Brazil      68361     926    8083.066602 2021-01-01\n",
       "3       Amazonas         Brazil     201574    5295    4863.536793 2021-01-01\n",
       "4          Bahia         Brazil     494684    9159    3326.039611 2021-01-01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19710, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quantidade de linhas faz sentido (27 estados x 730 dias)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19710 entries, 0 to 19709\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   Province_State  19710 non-null  object        \n",
      " 1   Country_Region  19710 non-null  object        \n",
      " 2   Confirmed       19710 non-null  int64         \n",
      " 3   Deaths          19710 non-null  int64         \n",
      " 4   Incident_Rate   19710 non-null  float64       \n",
      " 5   Date            19710 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(2)\n",
      "memory usage: 924.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Não temos dados nulos e os data types\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Wrangling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ajustando o nome das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mudando o nome das colunas Province_State e Country_Region\n",
    "df = df.rename(\n",
    "    columns=\n",
    "    {\n",
    "        'Province_State' : 'state',\n",
    "        'Country_Region' : 'country'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passando o nome de todas as colunas para minúsculo\n",
    "df.columns = [col.lower() for col in df.columns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ajustando o nome dos estados\n",
    "\n",
    "(Importante para o Looker Studio reconhece-los como uma localização geográfica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Acre', 'Alagoas', 'Amapá', 'Amazonas', 'Bahia', 'Ceará',\n",
       "       'Distrito Federal', 'Espírito Santo', 'Goiás', 'Maranhão',\n",
       "       'Mato Grosso', 'Mato Grosso do Sul', 'Minas Gerais', 'Pará',\n",
       "       'Paraíba', 'Paraná', 'Pernambuco', 'Piauí', 'Rio Grande do Norte',\n",
       "       'Rio Grande do Sul', 'Rio de Janeiro', 'Rondônia', 'Roraima',\n",
       "       'Santa Catarina', 'São Paulo', 'Sergipe', 'Tocantins'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando um mapper com os nomes\n",
    "mapper = {\n",
    "    'Amapa': 'Amapá',\n",
    "    'Ceara': 'Ceará',\n",
    "    'Espirito Santo': 'Espírito Santo',\n",
    "    'Goias': 'Goiás',\n",
    "    'Para': 'Pará',\n",
    "    'Paraiba': 'Paraíba',\n",
    "    'Parana': 'Paraná',\n",
    "    'Piaui': 'Piauí',\n",
    "    'Maranhao' : 'Maranhão',\n",
    "    'Rondonia': 'Rondônia',\n",
    "    'Sao Paulo': 'São Paulo'\n",
    "}\n",
    "\n",
    "# Aplicando o mapper na variável 'state'\n",
    "df['state'] = df['state'].apply(lambda state: mapper.get(state) if state in mapper.keys() else state)\n",
    "\n",
    "# Mostrando o resultado da operação\n",
    "df['state'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criando chaves temporais: Ano-Mês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df['date'].map(lambda date: date.strftime('%Y-%m'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Criando chaves temporais: Ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['date'].map(lambda date: date.strftime('%Y'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Estimando a população de cada Estado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos que a incidencia (`incident rate`) é uma estatística que mede a ocorrência de novos casos baseado na população total que está exposta ao risco. Como sabemos que esta taxa, neste estudo, está baseada em **100.000** habitantes, podemos encontrar esses valores da seguinte maneira:\n",
    "\n",
    "$$incident\\hspace{1mm}rate = \\dfrac{100.000 \\times confirmed}{population}$$\n",
    "\n",
    "Analogamente, conseguimos estimar a população total da seguinte maneira:\n",
    "\n",
    "\n",
    "$$population = \\dfrac{100.000 \\times confirmed}{incident\\hspace{1mm}rate}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['population'] = round(100000 * (df['confirmed'] / df['incident_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo a coluna incident_rate, já que ela nos dá pouca informação\n",
    "df = df.drop(['incident_rate'], axis= 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trabalhando com número de casos confirmados:\n",
    "\n",
    "1. Casos diários\n",
    "\n",
    "2. Média Móvel (7 dias)\n",
    "\n",
    "3. Estabilidade (14 dias)\n",
    "\n",
    "4. Definindo a tendencia da doença (crescente, decrescente ou estável) com base na estabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tendencia(rate: float) -> str:\n",
    "    \n",
    "    \"\"\"\n",
    "    Função criada para determinar a tendencia de desenvolvimento da doença.\\n\n",
    "    Caso a taxa seja maior que 1.15, a contaminação é considerada crescente.\\n\n",
    "    Caso a taxa seja menor que 0.75, a contaminação é considerada decrescente.\\n\n",
    "    Para outras situações, considera-se a mesma como estável.\n",
    "    \\n\n",
    "    >> Input\\n\n",
    "    rate (float): Razão entre a média móvel atual com a média móvel anterior.\n",
    "    \\n\n",
    "    >> Output\\n\n",
    "    status(str): Tendencia de desenvolvimento da doença.\n",
    "    \"\"\"\n",
    "    if np.isnan(rate):\n",
    "        return np.NaN\n",
    "    \n",
    "    if rate <= 0.75:\n",
    "        status = 'downward'\n",
    "    elif rate <= 1.15:\n",
    "        status = 'upward'\n",
    "    else:\n",
    "        status = 'stable'\n",
    "    \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cases_wrangled = pd.DataFrame()\n",
    "\n",
    "for state in df['state'].unique():\n",
    "    \n",
    "    df_queried = (\n",
    "        df\n",
    "        .query('state == @state') # Filtra pelo estado\n",
    "        .reset_index(drop= True) # Arruma a numeração das linhas\n",
    "        .sort_values(by= 'date') # Ordena pela data\n",
    "        .copy() # Cria uma cópia, evitando que o original seja alterado\n",
    "    )\n",
    "    \n",
    "    # Calculando o número de casos/mortes por dia\n",
    "    df_queried['confirmed_1d'] = df_queried['confirmed'].diff(periods= 1)\n",
    "    df_queried['deaths_1d'] = df_queried['deaths'].diff(periods = 1)\n",
    "    \n",
    "    # Calculando a média móvel de casos/mortes dos últimos 7 dias\n",
    "    df_queried['confirmed_moving_avg_7d'] = np.ceil(df_queried['confirmed_1d'].rolling(window= 7).mean())\n",
    "    df_queried['deaths_moving_avg_7d'] = np.ceil(df_queried['deaths_1d'].rolling(window= 7).mean())\n",
    "    \n",
    "    # Calculando a taxa de desenvolvimento da doença (comparando a média móvel atual com a anterior)\n",
    "    df_queried['confirmed_moving_avg_7d_rate_14'] = df_queried['confirmed_moving_avg_7d']/df_queried['confirmed_moving_avg_7d'].shift(periods= 14)\n",
    "    df_queried['deaths_moving_avg_7d_rate_14'] = df_queried['deaths_moving_avg_7d']/df_queried['deaths_moving_avg_7d'].shift(periods= 14)\n",
    "    \n",
    "    # Adicionando a coluna com o indicador do desenvolvimento da doença\n",
    "    df_queried['confirmed_trend'] = df_queried['confirmed_moving_avg_7d_rate_14'].apply(tendencia)\n",
    "    df_queried['deaths_trend'] = df_queried['deaths_moving_avg_7d_rate_14'].apply(tendencia)\n",
    "    \n",
    "    df_cases_wrangled = pd.concat([df_cases_wrangled, df_queried], axis= 0, ignore_index= True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
